\subsubsection{Allgemein}
Um Webcrawler analysieren zu können wurde eine kleine Webseite erstellt und auf einem Raspberry Pi\footnote{Ein Raspberry Pi ist ein sogenannter Einplatinencomputer, sprich ein Computer dessen gesamte Komponenten auf einer einzigen Platine zusammengelötet wurden. Ein Raspberry Pi zeichnet sich durch ein ausgezeichnetes Preis/Leistungsverhältnis mit einer für die meisten Anforderungen ausreichende Leistung bei einem moderaten Preis von ca. 50\euro\space aus. Er wurde für den Dauerbetrieb entwickelt und eignet sich deshalb bestens für einen Webserver.} betrieben. Als Webserver wurde hierbei Apache verwendet, da es sich durch die weite Verbreitung und eine Vielzahl an hinzufügbaren Erweiterungen bestens eignet. Da sowohl Apache als auch dessen Erweiterungsmodule Open Source sind, sprich der Quelltext ist öffentlich zugänglich, stehen sie kostenlos zum Download bereit.\\
%TODO: evtl Linux als BS erwähnen
Die Webseite wurde größtenteils in PHP verfasst. PHP ist eine relativ alte, aber dennoch weit verbreitete Skript-Sprache mit der sich dynamische Webseiten generieren lassen.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=8.45cm]{img/logo-webseite.png}
	\caption{Das Logo der Webseite}
	\label{fig:logo_webseite}
\end{figure}
Die Webseite trägt den Namen \emph{VEVETA} und ist öffentlich unter der Adresse \url{http://maturaprojekt.ddns.net} erreichbar. In Abbildung \ref{fig:logo_webseite} ist das Logo der Webseite abgebildet.\footnote{Das Logo wurde auf der Internetseite \url{http://logomakr.com} erstellt.} Es zeigt neben dem Namen der Webseite auch eine Spinne, welche an einem Browserfenster herunterhängt. Die Spinne steht hierbei symbolisch für eine Webspider, die gerade dabei ist eine Webseite zu analysieren. Die Abkürzung \emph{VEVETA} steht für \underline{VE}rbund \underline{VE}rschiedener \underline{TA}rpits und vermittelt somit sofort den Zweck dieser Webseite: Drei verschiedene Tarpitformen werden implementiert, um gleichzeitig drei verschiedene Arten von Webcrawlern anlocken und fangen zu können:
\begin{description}
	\item[Hyperlink-Tarpit] Hierbei werden verschiedene Hyperlinks, wie unter Punkt \ref{subsub:hyperlink-tarpit} beschrieben, ausgegeben, um somit jegliche Art von Webcrawlern anzulocken.
	\item[Harvester-Tarpit] Hierbei werden verschiedene zufällig generierte\\E-Mail-Adressen ausgegeben um Harvester anzulocken. Nähere Details hierzu sind unter Punkt \ref{subsub:harverster-tarpit} aufgeführt.
	\item[Brute-Force-Tarpit] Hier wird dem Aufrufer eine Log-In-Maske dargestellt. Jede Eingabe wird hierbei als \glqq falsch\grqq\space gewertet und mitgeloggt. Einzelheiten dazu sind unter Punkt \ref{subsub:brufe-force-tarpit} einsehbar.
\end{description}
Um den Verlauf der Tarpit besser beobachten zu können wurden kontinuierlich Logfiles\footnote{Ein Logfile ist eine Datei in der eine Software, je nach Konfiguration, verschiedene Informationen, speichert, um diese zu einem späteren Zeitpunkt auswerten zu können. Somit können Rückschlüsse auf Fehler, Auslastung und der gleichen gezogen werden.} geschrieben. Apache schreibt hierbei eigenständig einen Accesslog in welchem jeder Aufruf der Webseite mit IP-Adresse, Zeit, Adresse der aufgerufenen Seite, Referring Site\footnote{Eine Referring Site ist eine Webseite welche vor dem Aufruf der eigentlichen Webseite aufgerufen wurde, sprich es ist diese Webseite von der aus ein Aufrufer über einen Link auf eine andere, \glqq meine\grqq, Webseite gelangt ist.} und User-Agent notiert wird. Um diese Logfile kontinuierlich auswerten zu können, wurde die Open Source Drittanbieter-Software \emph{GoAcces\footnote{Nähere Informationen, Downloadlinks, Live-Demo, Dokumentation, etc. sind auf der \href{https://goaccess.io/}{\emph{Homepage}} von GoAccess (\url{https://goaccess.io/}) angeführt.}} verwendet, welche Accesslogs auswertet und, unter anderem live, Statistiken über den Traffic\footnote{Als \emph{Traffic} einer Webseite bezeichnet man den anfallenden Datenverkehr, welcher beispielsweise durch einen Aufruf einer Seite entsteht.} der Webseite ausgibt.\\
Um alle Webcrawler vor den Gefahren zu warnen, muss die Datei robots.txt um die folgenden Zeilen entsprechend angepasst werden.
\begin{lstlisting}
	User-agent: *
		Disallow: /
\end{lstlisting}
Hiermit wird allen Webcrawlern empfohlen, die komplette Webseite zu meiden.\\
Alle Dateien, welche im Verlauf dieser Arbeit entstanden sind, können online auf GitHub eingesehen werden:\\\url{https://github.com/Analyse-von-Webcrawlern}.
\label{umsetzung_grundlegendes}