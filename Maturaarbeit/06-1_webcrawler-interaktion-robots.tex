Das Robots Exclusion Protocol, besser bekannt als \emph{robots.txt} wurde 1994\cite{robots-date} vom Holländer Martijn Koster\cite{robots-martijn} erfunden und hat sich zum de-facto-Standard in Sachen Interaktion mit Webcrawlern entwickelt, obwohl es nie ein offizieller Standard wurde.\cite{robots-date}\\
Um das Robots Exclusion Protocol zu verwenden, muss eine Datei mit dem Namen \emph{robots.txt} im Wurzelverzeichnis der Webseite abgespeichert werden. In dieser Datei können dann Regeln festgelegt werden, die besagen, welcher Webcrawler welche Dateien und Ordner besuchen darf bzw. welche nicht. Das einhalten dieser Regeln ist jedoch fakultativ, weshalb Webcrawler die Regeln der robots.txt auch einfach ignorieren können.