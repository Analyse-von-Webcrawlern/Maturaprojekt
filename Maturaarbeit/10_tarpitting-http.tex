\subsubsection{HTTP-Tarpit}
HTTP-Tarpits sind neben SMTP-Tarpits eine der am häufigsten eingesetzten Tarpits. Sie haben aber weniger das Ziel den Gegenüber auszubremsen, sondern ihn eine möglichst lange Zeit zu \glqq beschäftigen\grqq , indem sie ihm stets neue unsinnige Informationen liefern.\\
Wie aus Abbildung \ref{fig:arbeitsweise_crawler}, Abschnitt \ref{subsub:funk}, erkennbar ist, besitzen Webcrawler eine Queue, in welcher die Adressen aller noch zu besuchenden Webseiten vermerkt sind. Ziel einer HTTP-Tarpit ist es nun, diese Queue kontinuierlich mit sinnlosen Adressen zu befüllen, damit ein gefangener Webcrawler möglichst viel Zeit damit verbringt, diese Adressen zu besuchen. Die aufgebrachte Zeit ist für den Webcrawler verschwendete Zeit. Wie bereits vorhin erwähnt ist Zeit jedoch eine der wichtigsten Ressourcen eines Webcrawlers. Meist zeigen die generierten Adressen auf die Tarpit selber und somit beginnt der Prozess von neuem. Ausgereifte Tarpits sind hierbei in der Lage Webcrawler für mehrere Tage zu beschäftigen.\cite{tarpitting-http-linux-mag}\\
Eine HTTP-Tarpit ist auch in der Lage Harvester in ihrer Arbeit zu behindern. Hierzu werden einige der generierten Adressen durch E-Mail-Adressen ausgetauscht. Um die E-Mail-Adressen für den Harvester sichtbar zu machen, muss im HTML-Quelltext beim Hyperlinktag \emph{<a>} bei der Zieladresse der entsprechende Präfix \emph{mailto:} gesetzt werden. Der Tag sieht dann folgendermaßen aus: \emph{<a href:"mailto:jemand@example.com">jemand@example.com<a/>}.\\
Diese verbreiteten E-Mail-Adressen sind entweder \glqq tot\grqq, sprich sie führen zu keinem E-Mail-Server oder \glqq infiziert\grqq, sprich sie zeigen auf einen präparierten E-Mail-Server der beispielsweise SMTP-Tarpitting betreibt.\\
Obwohl die Dokumentation solcher Tarpits bewusst geheim gehalten wird, rüsten Entwickler von Webcrawlern ihre Programme kontinuierlich gegen solche Fallen. Sie limitieren beispielsweise die Anzahl der auf einer Domaine aufgerufenen Seiten oder bauen andere Abwehrmaßnahmen gegen Tarpits ein.\\
Zu bedenken ist auch, dass der Besitzer einer Tarpit hier absichtlich Ressourcen verschwendet, indem er bewusst bösartige Webcrawler auf dem Webserver hält. Mit jedem neuen Webcrawler steigt des Weiteren die Gefahr eines Denial of Service, sprich ein Zusammenbruch des Servers aufgrund von Überlastung, drastisch.\cite{tarpitting-http-eggendorf-auugn}\\
Auch ist die Frage nach der Legalität nicht vollständig geklärt. Rechtlich bewegt man sich mit dem Betreiben einer Tarpit in einer Grauzone. Um jedoch sich selbst abzusichern, sollte man mithilfe von robots.txt\footnote{Mehr zum Thema \emph{robots.txt} ist im Kapitel \ref{subsub:funk} auf Seite \pageref{subsub:funk} aufgezeigt.} die Webcrawler \glqq warnen\grqq , indem man sie ausschließt. Sollten Webcrawler dann trotzdem in die Tarpit geraten wurden sie vorher gewarnt und sind demnach \glqq selbst schuld\grqq\ \cite{honeypot-tarpit-synonym}. Somit ist auch gewährleistet, dass keine gutwilligen Webcrawler, wie etwa GoogleBot\footnote{Der Webcrawler GoogleBot wird in Kapitel \ref{subsub:googlebot} auf Seite  \pageref{subsub:googlebot} genauer beschrieben.}, welche sich an die Regeln von robots.txt halten, von der Tarpit betroffen sind\cite{tarpitting-http-eggendorf-auugn}.
\label{subsub:http-tarpit}